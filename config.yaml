model_type: 'TrainPolicy' #{TrainPolicy, TrainRewardModel, UseHumanFeedback}
train_dataset: './dataset/GSPC.csv'     # Stock File
test_dataset: './dataset/GSPC_test.csv'
window_size: 32    # Window length of the previous market position  

seed: 12345
episode: 1000     # number of training episodes 
obs_dim: 32       # Observation dimesion
act_dim: 3        # Action Dimension
device: 'cpu'     # Device {'cpu' or 'cuda'}

agent:
  classname: traderl.agent.Agent
  obs_dim: ${obs_dim}
  act_dim: ${act_dim}
  batch_size: 128
  gamma: 0.99
  eps_start: 0.9
  eps_end: 0.05
  eps_decay: 1000
  tau: 0.005
  lr: 0.0001
  capacity: 10000
  device: ${device}

train:
  stock: ${train_datset}
  window_size: ${window_size}
  episode: ${episode}
  obs_dim: ${obs_dim}
  act_dim: ${act_dim}
  agent: ${agent}
  use_HF: False
  reward_model: None
  device: ${device}
  agent_path: None 

rewardmodel:
  human_feedback: './dataset/GSPC_human_feedback.csv'
  lr: 0.001
  epochs: 300
  verbose: 2
  stock_name: 'GPSC'